# Going Deeper프로젝트에서 이론에 해당하는 내용입니다.
### [GD1. 텍스트데이터다루기](https://foul-beechnut-069.notion.site/GD1-fa2a962631b34a77b9e4bfdb006124f4)
  - Preprocessing : 자연어의 노이즈 제거
  - 분산표현 : 바나나와 사과의 관계를 어떻게 표현할까?
  - 토큰화 : 그녀는? 그녀+는?  
  - 토큰화 : 다른 방법들
  - 토큰에게 의미를 부여하기
  - 마무리
### [GD2.텍스트의 분포로 벡터화 하기](https://foul-beechnut-069.notion.site/GD2-a8aec887008c4ff3b4129734662052ee)
  - 단어 빈도를 이용한 벡터화
  - LSA와 LDA
  - 텍스트 분포를 이용한 비지도 학습 토크나이저
### [GD3.워드임베딩](https://foul-beechnut-069.notion.site/GD3-7f00360c6d84425cb08a736e2150ff9d)
  - Vectorization
  - 원-핫 인코딩 구현해보기
  - 워드 임베딩
  - Word2Vec : 분포 가설
  - Word2Vec : CBoW
  - Word2Vec : Skip-gram과 Negative Sampling
  - Word2Vec : 영어 word2vec실습과 OOV문제
  - FastText
  - GloVe(Global Vectors for Word Representation)
### [GD5.Transformer가 나오기까지](https://foul-beechnut-069.notion.site/GD5-Transformer-21bd28eecaa34f0cba83a5e2abd7440a)
  - Attention의 역사
  - Positional Encoding
  - Multi-Head Attention
### [GD6.기계 번역이 걸어온 길](https://foul-beechnut-069.notion.site/GD6-4b6980b12b89490186e965f8bf80f9ae)
  - 번역의 흐름
  - 문장 생성 방식
  - Data Augmentation
  - 성능 평가 지표, BLEU score
  - 챗봇과 번역기
  - 좋은 챗봇이 되려면
### [GD7.modern NLP의 흐름에 올라타보자.](https://foul-beechnut-069.notion.site/GD7-modern-NLP-65e0743add6e4478b4c378c347818ba8)
  - ELMo(Embedding from Language Models)
  - GPT(Generative Pre-Training Transformer)
  - Transformer-XL(Transformer Extra Long)
  - XLNet, BART
  - ALBERT(A Lite BERT for Self-supervised Learning of Language Representations)
  - T5(Text-to-Text Transfer Transformer)
  - Switch Transformer
  - ERNIE
### [GD8.NLP Framework의 활용](https://foul-beechnut-069.notion.site/GD8-NLP-Framework-93993b869d9740c3885dd008707a6f6b)
  - 다양한 NLP Framework
  - Huggingface transformers 개요
  - Huggingface transformers : Model
  - Huggingface transformers : Tokenizer
  - Huggingface transformers : Processor
  - Huggingface transformers : Config
  - Huggingface transformers : Trainer
